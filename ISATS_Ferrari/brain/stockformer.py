"""
ğŸ“ˆ ISATS v6.0 - Stockformer (ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ì‘ì „ëª…: "Transformer + 1D-CNN ê¸°ë°˜ ì£¼ê°€ ì˜ˆì¸¡"

ì—­í• :
- Transformer êµ¬ì¡°ë¡œ ì‹œê°„ì  íŒ¨í„´ í•™ìŠµ
- 1D-CNNìœ¼ë¡œ ì§€ì—­ì  íŠ¹ì§• ì¶”ì¶œ
- Granger ì¸ê³¼ê´€ê³„ ê¸°ë°˜ ë‹¤ë³€ëŸ‰ ì…ë ¥
- ë‹¤ìŒ 5ì¼ ì£¼ê°€ ì˜ˆì¸¡

ì‘ì„±ì: ISATS Neural Swarm
ë²„ì „: 6.0 (Stockformer)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""

import os
import sys
import numpy as np
import pandas as pd
from typing import List, Tuple, Optional
from datetime import datetime, timedelta

# í”„ë¡œì íŠ¸ ë£¨íŠ¸
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ì„ íƒì  ì„í¬íŠ¸
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import Dataset, DataLoader
    HAS_TORCH = True
except ImportError:
    HAS_TORCH = False
    print("âš ï¸ [Warning] PyTorch not found. Installing...")
    os.system("pip install torch --quiet")
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import Dataset, DataLoader
    HAS_TORCH = True

try:
    from sklearn.preprocessing import MinMaxScaler
    HAS_SKLEARN = True
except ImportError:
    HAS_SKLEARN = False
    print("âš ï¸ [Warning] scikit-learn not found. Installing...")
    os.system("pip install scikit-learn --quiet")
    from sklearn.preprocessing import MinMaxScaler
    HAS_SKLEARN = True


# ==========================================
# ğŸ“Š ë°ì´í„°ì…‹
# ==========================================

class StockDataset(Dataset):
    """ì£¼ê°€ ì‹œê³„ì—´ ë°ì´í„°ì…‹"""
    
    def __init__(
        self,
        data: pd.DataFrame,
        seq_length: int = 60,
        pred_length: int = 5
    ):
        """
        Args:
            data: OHLCV ë°ì´í„°í”„ë ˆì„
            seq_length: ì…ë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´ (60ì¼)
            pred_length: ì˜ˆì¸¡ ê¸¸ì´ (5ì¼)
        """
        self.seq_length = seq_length
        self.pred_length = pred_length
        
        # ì •ê·œí™”
        self.scaler = MinMaxScaler()
        self.data = self.scaler.fit_transform(data.values)
        
        # ì‹œí€€ìŠ¤ ìƒì„±
        self.sequences = []
        self.targets = []
        
        for i in range(len(self.data) - seq_length - pred_length):
            seq = self.data[i:i + seq_length]
            target = self.data[i + seq_length:i + seq_length + pred_length, 3]  # Close ê°€ê²©
            
            self.sequences.append(seq)
            self.targets.append(target)
        
        self.sequences = np.array(self.sequences)
        self.targets = np.array(self.targets)
    
    def __len__(self):
        return len(self.sequences)
    
    def __getitem__(self, idx):
        return (
            torch.FloatTensor(self.sequences[idx]),
            torch.FloatTensor(self.targets[idx])
        )


# ==========================================
# ğŸ§  Stockformer ëª¨ë¸
# ==========================================

class Stockformer(nn.Module):
    """Transformer + 1D-CNN ê¸°ë°˜ ì£¼ê°€ ì˜ˆì¸¡ ëª¨ë¸"""
    
    def __init__(
        self,
        input_dim: int = 5,  # OHLCV
        d_model: int = 128,
        nhead: int = 8,
        num_layers: int = 3,
        pred_length: int = 5
    ):
        """
        Args:
            input_dim: ì…ë ¥ ì°¨ì› (OHLCV = 5)
            d_model: Transformer ì„ë² ë”© ì°¨ì›
            nhead: Multi-head Attention í—¤ë“œ ìˆ˜
            num_layers: Transformer ë ˆì´ì–´ ìˆ˜
            pred_length: ì˜ˆì¸¡ ê¸¸ì´
        """
        super(Stockformer, self).__init__()
        
        self.input_dim = input_dim
        self.d_model = d_model
        self.pred_length = pred_length
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # 1D-CNN (ì§€ì—­ì  íŠ¹ì§• ì¶”ì¶œ)
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        
        self.conv1 = nn.Conv1d(input_dim, 64, kernel_size=3, padding=1)
        self.conv2 = nn.Conv1d(64, d_model, kernel_size=3, padding=1)
        self.relu = nn.ReLU()
        self.pool = nn.MaxPool1d(2)
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # Transformer Encoder
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=512,
            dropout=0.1,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # ì¶œë ¥ ë ˆì´ì–´
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        
        self.fc1 = nn.Linear(d_model, 64)
        self.fc2 = nn.Linear(64, pred_length)
        self.dropout = nn.Dropout(0.1)
    
    def forward(self, x):
        """
        Args:
            x: (batch_size, seq_length, input_dim)
        
        Returns:
            (batch_size, pred_length)
        """
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # 1D-CNN
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        
        # (batch, seq, input_dim) -> (batch, input_dim, seq)
        x = x.permute(0, 2, 1)
        
        x = self.relu(self.conv1(x))
        x = self.relu(self.conv2(x))
        
        # (batch, d_model, seq) -> (batch, seq, d_model)
        x = x.permute(0, 2, 1)
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # Transformer
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        
        x = self.transformer(x)
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # ì¶œë ¥ (ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í…ë§Œ ì‚¬ìš©)
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        
        x = x[:, -1, :]  # (batch, d_model)
        x = self.dropout(self.relu(self.fc1(x)))
        x = self.fc2(x)  # (batch, pred_length)
        
        return x


# ==========================================
# ğŸ“ í•™ìŠµ ë° ì˜ˆì¸¡
# ==========================================

class StockformerTrainer:
    """Stockformer í•™ìŠµ ë° ì˜ˆì¸¡"""
    
    def __init__(
        self,
        model: Stockformer,
        device: str = "cpu"
    ):
        self.model = model.to(device)
        self.device = device
        self.criterion = nn.MSELoss()
        self.optimizer = optim.Adam(model.parameters(), lr=0.001)
    
    def train(
        self,
        train_loader: DataLoader,
        epochs: int = 50,
        verbose: bool = True
    ):
        """
        ëª¨ë¸ í•™ìŠµ
        
        Args:
            train_loader: í•™ìŠµ ë°ì´í„° ë¡œë”
            epochs: ì—í­ ìˆ˜
            verbose: ë¡œê·¸ ì¶œë ¥ ì—¬ë¶€
        """
        self.model.train()
        
        for epoch in range(epochs):
            total_loss = 0
            
            for batch_x, batch_y in train_loader:
                batch_x = batch_x.to(self.device)
                batch_y = batch_y.to(self.device)
                
                # Forward
                pred = self.model(batch_x)
                loss = self.criterion(pred, batch_y)
                
                # Backward
                self.optimizer.zero_grad()
                loss.backward()
                self.optimizer.step()
                
                total_loss += loss.item()
            
            if verbose and (epoch + 1) % 10 == 0:
                avg_loss = total_loss / len(train_loader)
                print(f"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.6f}")
    
    def predict(self, x: torch.Tensor) -> np.ndarray:
        """
        ì˜ˆì¸¡
        
        Args:
            x: ì…ë ¥ ì‹œí€€ìŠ¤ (seq_length, input_dim)
        
        Returns:
            ì˜ˆì¸¡ê°’ (pred_length,)
        """
        self.model.eval()
        
        with torch.no_grad():
            x = x.unsqueeze(0).to(self.device)  # (1, seq_length, input_dim)
            pred = self.model(x)
            return pred.cpu().numpy()[0]
    
    def save(self, path: str):
        """ëª¨ë¸ ì €ì¥"""
        torch.save(self.model.state_dict(), path)
        print(f"âœ… ëª¨ë¸ ì €ì¥: {path}")
    
    def load(self, path: str):
        """ëª¨ë¸ ë¡œë“œ"""
        self.model.load_state_dict(torch.load(path))
        print(f"âœ… ëª¨ë¸ ë¡œë“œ: {path}")


# ==========================================
# ì‹¤í–‰
# ==========================================

if __name__ == "__main__":
    print(f"\n{'='*80}")
    print(f"ğŸ“ˆ Stockformer í…ŒìŠ¤íŠ¸")
    print(f"{'='*80}\n")
    
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # 1. ë°ì´í„° ë¡œë“œ
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    
    # ì˜ˆì‹œ: ì‚¼ì„±ì „ì ë°ì´í„°
    data_path = "data/KR/005930.KS.csv"
    
    if os.path.exists(data_path):
        df = pd.read_csv(data_path)
        df = df[['Open', 'High', 'Low', 'Close', 'Volume']].dropna()
        
        print(f"âœ… ë°ì´í„° ë¡œë“œ: {len(df)}ê°œ ë ˆì½”ë“œ")
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # 2. ë°ì´í„°ì…‹ ìƒì„±
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        
        dataset = StockDataset(df, seq_length=60, pred_length=5)
        train_loader = DataLoader(dataset, batch_size=32, shuffle=True)
        
        print(f"âœ… ë°ì´í„°ì…‹ ìƒì„±: {len(dataset)}ê°œ ì‹œí€€ìŠ¤")
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # 3. ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        
        model = Stockformer(input_dim=5, d_model=128, nhead=8, num_layers=3, pred_length=5)
        trainer = StockformerTrainer(model, device="cpu")
        
        print(f"\nğŸ“ ëª¨ë¸ í•™ìŠµ ì‹œì‘...\n")
        trainer.train(train_loader, epochs=50, verbose=True)
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # 4. ì˜ˆì¸¡
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        
        last_seq = torch.FloatTensor(dataset.sequences[-1])
        prediction = trainer.predict(last_seq)
        
        print(f"\n{'='*80}")
        print(f"ğŸ“Š ì˜ˆì¸¡ ê²°ê³¼ (ë‹¤ìŒ 5ì¼)")
        print(f"{'='*80}")
        print(f"ì˜ˆì¸¡ê°’: {prediction}")
        print(f"{'='*80}\n")
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # 5. ëª¨ë¸ ì €ì¥
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        
        trainer.save("brain/stockformer_model.pth")
    
    else:
        print(f"âŒ ë°ì´í„° íŒŒì¼ ì—†ìŒ: {data_path}")
        print(f"   ë¨¼ì € utils/universal_data_collector.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
